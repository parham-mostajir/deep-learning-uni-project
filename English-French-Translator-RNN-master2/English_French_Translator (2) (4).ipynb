{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2pukCDIb7yT"
   },
   "source": [
    "# Machine Learning Translation\n",
    "In this project we aim to convert English phrases to French using RNN on Deep Learning Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1wmao4O0cnLS",
    "outputId": "11ac912b-538b-49d9-e0c5-25964927e4df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and preprocess data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[english](https://raw.githubusercontent.com/projjal1/datasets/master/small_vocab_en.txt)\n",
    "\n",
    "[french](https://raw.githubusercontent.com/projjal1/datasets/master/small_vocab_fr.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cboaEhaZcvm3"
   },
   "outputs": [],
   "source": [
    "english_path='https://raw.githubusercontent.com/projjal1/datasets/master/small_vocab_en.txt'\n",
    "french_path='https://raw.githubusercontent.com/projjal1/datasets/master/small_vocab_fr.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnOhjwXI1VYO"
   },
   "source": [
    "#### Load the dataset and split file by lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ozqkItv1Pd6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "  input_file = os.path.join(path)\n",
    "  with open(input_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "  return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-jmqlMhdC0H"
   },
   "outputs": [],
   "source": [
    "english_data=tf.keras.utils.get_file('file1',english_path)\n",
    "french_data=tf.keras.utils.get_file('file2',french_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crFQWCx2eLsv"
   },
   "outputs": [],
   "source": [
    "english_sentences=load_data(english_data)\n",
    "french_sentences=load_data(french_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw60fkIXgCB4"
   },
   "source": [
    "\n",
    "#### few examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "9V_5yFcKgI-4",
    "outputId": "bb5e0ca3-679a-444b-af29-c68ed512a72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample : 0\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "--------------------------------------------------\n",
      "Sample : 1\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n",
      "--------------------------------------------------\n",
      "Sample : 2\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "california est gÃ©nÃ©ralement calme en mars , et il est gÃ©nÃ©ralement chaud en juin .\n",
      "--------------------------------------------------\n",
      "Sample : 3\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et il fait froid en septembre .\n",
      "--------------------------------------------------\n",
      "Sample : 4\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "votre moins aimÃ© fruit est le raisin , mais mon moins aimÃ© est la pomme .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print('Sample :',i)\n",
    "  print(english_sentences[i])\n",
    "  print(french_sentences[i])\n",
    "  print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EV6ri_Shkff"
   },
   "source": [
    "#### Convert to Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_U3gqv5WhOTy",
    "outputId": "2748311c-d8a4-433e-a86c-545c97f616cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab: 227\n",
      "French Vocab: 355\n",
      "Counter({'is': 205858, ',': 140897, '.': 129039, 'in': 75525, 'it': 75137, 'during': 74933, 'the': 67628, 'but': 63987, 'and': 59850, 'sometimes': 37746, 'usually': 37507, 'never': 37500, 'least': 27564, 'favorite': 27371, 'fruit': 27105, 'most': 14934, 'loved': 13666, 'liked': 13546, 'new': 12197, 'paris': 11334, 'india': 11277, 'united': 11270, 'states': 11270, 'california': 11250, 'jersey': 11225, 'france': 11170, 'china': 10953, 'he': 10786, 'she': 10786, 'grapefruit': 10118, 'your': 9734, 'my': 9700, 'his': 9700, 'her': 9700, 'fall': 9134, 'june': 9133, 'spring': 9102, 'january': 9090, 'winter': 9038, 'march': 9023, 'autumn': 9004, 'may': 8995, 'nice': 8984, 'september': 8958, 'july': 8956, 'april': 8954, 'november': 8951, 'summer': 8948, 'december': 8945, 'february': 8942, 'our': 8932, 'their': 8932, 'freezing': 8928, 'pleasant': 8916, 'beautiful': 8915, 'october': 8910, 'snowy': 8898, 'warm': 8890, 'cold': 8878, 'wonderful': 8808, 'dry': 8794, 'busy': 8791, 'august': 8789, 'chilly': 8770, 'rainy': 8761, 'mild': 8743, 'wet': 8726, 'relaxing': 8696, 'quiet': 8693, 'hot': 8639, 'dislikes': 7314, 'likes': 7314, 'limes': 5554, 'mangoes': 5549, 'lemons': 5533, 'grapes': 5525, 'apples': 5452, 'oranges': 5452, 'strawberries': 5452, 'bananas': 5452, 'peaches': 5451, 'pears': 5451, 'to': 5166, 'strawberry': 4715, 'grape': 4703, 'lime': 4680, 'apple': 4652, 'lemon': 4652, 'banana': 4652, 'mango': 4652, 'pear': 4652, 'peach': 4652, 'orange': 4651, 'like': 4588, 'dislike': 4444, 'they': 3222, 'that': 2712, 'i': 2664, 'we': 2532, 'you': 2414, 'animal': 2304, 'a': 1944, 'truck': 1944, 'car': 1944, 'automobile': 1944, 'was': 1867, 'next': 1666, 'go': 1386, 'driving': 1296, 'visit': 1224, 'little': 1016, 'big': 1016, 'old': 972, 'yellow': 972, 'red': 972, 'rusty': 972, 'blue': 972, 'white': 972, 'black': 972, 'green': 972, 'shiny': 972, 'favorite.': 961, 'are': 870, '?': 811, 'last': 781, 'feared': 768, 'animals': 768, 'this': 768, 'plan': 714, 'going': 666, 'saw': 648, 'disliked': 648, 'drives': 648, 'drove': 648, 'grapefruit.': 574, 'between': 540, 'liked.': 500, 'loved.': 500, 'translate': 480, 'plans': 476, 'peaches.': 393, 'pears.': 393, 'bananas.': 392, 'oranges.': 392, 'apples.': 392, 'strawberries.': 392, 'were': 384, 'went': 378, 'might': 378, 'wanted': 378, 'thinks': 360, 'grapes.': 319, 'spanish': 312, 'portuguese': 312, 'chinese': 312, 'english': 312, 'french': 312, 'lemons.': 311, 'translating': 300, 'mangoes.': 295, 'limes.': 290, 'difficult': 260, 'fun': 260, 'easy': 260, 'wants': 252, 'think': 240, 'why': 240, \"it's\": 240, 'did': 204, 'orange.': 197, 'mango.': 196, 'banana.': 196, 'peach.': 196, 'lemon.': 196, 'pear.': 196, 'apple.': 196, 'cat': 192, 'shark': 192, 'bird': 192, 'mouse': 192, 'horse': 192, 'elephant': 192, 'dog': 192, 'monkey': 192, 'lion': 192, 'bear': 192, 'rabbit': 192, 'snake': 192, 'lime.': 168, 'grape.': 145, 'when': 144, 'strawberry.': 133, 'want': 126, 'fruit.': 87, 'do': 84, 'how': 67, 'elephants': 64, 'horses': 64, 'dogs': 64, 'sharks': 64, 'snakes': 64, 'cats': 64, 'rabbits': 64, 'monkeys': 64, 'bears': 64, 'birds': 64, 'lions': 64, 'mice': 64, \"didn't\": 60, 'eiffel': 57, 'tower': 57, 'grocery': 57, 'store': 57, 'football': 57, 'field': 57, 'lake': 57, 'school': 57, 'would': 48, \"aren't\": 36, 'been': 36, 'weather': 33, 'does': 24, 'has': 24, \"isn't\": 24, 'am': 24, 'where': 12, 'have': 12})\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('English Vocab:',len(english_words_counter))\n",
    "print('French Vocab:',len(french_words_counter))\n",
    "print(english_words_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsl03eu-jW09"
   },
   "source": [
    "# Tokenize \n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYLtZ0WmjTjD"
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(x)\n",
    "  return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "lkhE2V5bjyg_",
    "outputId": "6c08f972-054a-48f5-d6d3-43cd93f0a801",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "  print('Sequence {} in x'.format(sample_i + 1))\n",
    "  print('  Input:  {}'.format(sent))\n",
    "  print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfiSCxQ7kRK3"
   },
   "source": [
    "# Padding \n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Daf_ngukNIS"
   },
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "  return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "9LHAL30ik5x4",
    "outputId": "f23ae132-73ae-4700-b34c-1f1d72c5fefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    #Expanding dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSLZv-O_3Bgq"
   },
   "source": [
    "## Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want. We want the French translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BDBJKw22yJa"
   },
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "  index_to_words[0] = '<PAD>'\n",
    "\n",
    "  #So basically we are predicting output for a given word and then selecting best answer\n",
    "  #Then selecting that label we reverse-enumerate the word from id\n",
    "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mbg5kGAY3cvT"
   },
   "source": [
    "![Model](https://github.com/tommytracey/AIND-Capstone/raw/8267d4fe72e48c595a0aff46eaf0a805fff0f36d/images/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HN-RjzvH3rpp"
   },
   "source": [
    "# Building Model\n",
    "Here we use RNN model combined with GRU nodes for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ta5yGpZ23amq"
   },
   "outputs": [],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    input_shape: Tuple of input shape\n",
    "    output_sequence_length: Length of output sequence\n",
    "    english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters نرخ یادگیری بهینه‌ساز Adam\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    # تبدیل کلمات انگلیسی به بردارهای 256 بعدی\n",
    "    model.add(GRU(256, return_sequences=True))  \n",
    "    # لایه بازگشتی با 256 واحد حافظه و حفظ خروجی برای هر تایم‌استپ\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    # لایه متراکم 1024 واحدی با ReLU برای هر تایم‌استپ\n",
    "    model.add(Dropout(0.5))\n",
    "    # جلوگیری از اورفیتینگ با غیرفعال کردن تصادفی 50% نورون‌ها\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "    # لایه خروجی که برای هر کلمه، احتمال تمام کلمات فرانسوی را محاسبه می‌کند\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubkWXBpu32Jh"
   },
   "outputs": [],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NXsCzht3-1y"
   },
   "source": [
    "## Finally calling the model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. پارامترهای ورودی تابع embed_model :\n",
    "tmp_x.shape: شکل داده‌های ورودی انگلیسی (بعد از پیش‌پردازش)\n",
    "\n",
    "معمولاً به شکل (تعداد جملات, حداکثر طول جمله) است\n",
    "\n",
    "preproc_french_sentences.shape[1]: طول جملات فرانسوی (پس از padding)\n",
    "\n",
    "len(english_tokenizer.word_index)+1: اندازه دایره لغات انگلیسی + ۱ (برای padding)\n",
    "\n",
    "len(french_tokenizer.word_index)+1: اندازه دایره لغات فرانسوی + ۱ (برای padding)\n",
    "\n",
    "### 2. ساختار مدل ایجاد شده:\n",
    "مدل ایجاد شده شامل این لایه‌هاست  :\n",
    "\n",
    "لایه Embedding:\n",
    "\n",
    "تبدیل کلمات انگلیسی به بردارهای متراکم\n",
    "مثال: \"cat\" → [0.2, -0.5, 0.7, ...] (بردار 256 بعدی)\n",
    "\n",
    "لایه GRU:\n",
    "\n",
    "256 واحد حافظه\n",
    "\n",
    "return_sequences=True برای حفظ خروجی در هر گام زمانی\n",
    "\n",
    "لایه‌های Dense با TimeDistributed :\n",
    "\n",
    "پردازش مستقل هر گام زمانی\n",
    "\n",
    "لایه نهایی با softmax برای پیش‌بینی احتمالات کلمات فرانسوی"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مثال :\n",
    "\"the weather is nice\"\n",
    "\n",
    "تبدیل هر کلمه به بردار Embedding\n",
    "\n",
    "پردازش توسط GRU\n",
    "\n",
    "تولید توزیع احتمال برای هر کلمه فرانسوی در خروجی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8B9QZgl738qJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "R-2dQpBQ4EZa",
    "outputId": "6bd4bcb1-e0a1-4fd8-a4c1-0d3878d971c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 21, 1024)          263168    \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 21, 346)           354650    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1063770 (4.06 MB)\n",
      "Trainable params: 1063770 (4.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMDG7ttA4JGS"
   },
   "source": [
    "# Training the model\n",
    "Here we start to train the model and pass the english text and the max_sequence_length, with vocab size for both english and french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "colab_type": "code",
    "id": "fY7037yU4HCp",
    "outputId": "91b77adb-02b0-4a31-b4dc-1bc3680563a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "108/108 [==============================] - 85s 771ms/step - loss: 1.2600 - accuracy: 0.7007 - val_loss: 0.4460 - val_accuracy: 0.8564\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 85s 788ms/step - loss: 0.3841 - accuracy: 0.8739 - val_loss: 0.2872 - val_accuracy: 0.9031\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 85s 786ms/step - loss: 0.2782 - accuracy: 0.9071 - val_loss: 0.2303 - val_accuracy: 0.9219\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 86s 799ms/step - loss: 0.2352 - accuracy: 0.9200 - val_loss: 0.2134 - val_accuracy: 0.9260\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 84s 782ms/step - loss: 0.2130 - accuracy: 0.9267 - val_loss: 0.1948 - val_accuracy: 0.9326\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 86s 793ms/step - loss: 0.2005 - accuracy: 0.9301 - val_loss: 0.1921 - val_accuracy: 0.9329\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 86s 795ms/step - loss: 0.1925 - accuracy: 0.9324 - val_loss: 0.1840 - val_accuracy: 0.9354\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 85s 784ms/step - loss: 0.1858 - accuracy: 0.9343 - val_loss: 0.1822 - val_accuracy: 0.9359\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 87s 802ms/step - loss: 0.1811 - accuracy: 0.9352 - val_loss: 0.1800 - val_accuracy: 0.9369\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 86s 792ms/step - loss: 0.1788 - accuracy: 0.9361 - val_loss: 0.1793 - val_accuracy: 0.9370\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 86s 796ms/step - loss: 0.1778 - accuracy: 0.9363 - val_loss: 0.1792 - val_accuracy: 0.9370\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 85s 788ms/step - loss: 0.1726 - accuracy: 0.9376 - val_loss: 0.1775 - val_accuracy: 0.9382\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 78s 719ms/step - loss: 0.1682 - accuracy: 0.9388 - val_loss: 0.1744 - val_accuracy: 0.9387\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 81s 748ms/step - loss: 0.1692 - accuracy: 0.9385 - val_loss: 0.1800 - val_accuracy: 0.9382\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 79s 730ms/step - loss: 0.1679 - accuracy: 0.9388 - val_loss: 0.1775 - val_accuracy: 0.9381\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 84s 779ms/step - loss: 0.1656 - accuracy: 0.9394 - val_loss: 0.1758 - val_accuracy: 0.9387\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 81s 749ms/step - loss: 0.1625 - accuracy: 0.9402 - val_loss: 0.1758 - val_accuracy: 0.9392\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 81s 748ms/step - loss: 0.1607 - accuracy: 0.9407 - val_loss: 0.1766 - val_accuracy: 0.9394\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 81s 747ms/step - loss: 0.1658 - accuracy: 0.9395 - val_loss: 0.1799 - val_accuracy: 0.9379\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 77s 716ms/step - loss: 0.1687 - accuracy: 0.9386 - val_loss: 0.1857 - val_accuracy: 0.9364\n"
     ]
    }
   ],
   "source": [
    "history=simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Um2UZ6EbSge9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lg0KtRlXBHPE"
   },
   "source": [
    "#  Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ys8_8FhABNfx"
   },
   "outputs": [],
   "source": [
    "def final_predictions(text):\n",
    "  y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "  y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "  sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
    "  sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
    "  \n",
    "  print(sentence.shape)\n",
    "  print(logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pJp2gkLYE0zZ",
    "outputId": "82d0935e-c51b-4a0b-b4dc-9659a32d5cda"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " he likes grape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "il aime le raisin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "txt=input().lower()\n",
    "final_predictions(re.sub(r'[^\\w]', ' ', txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "English-French-Translator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
