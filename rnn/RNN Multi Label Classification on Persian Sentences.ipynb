{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rG540utTBHVi"
   },
   "source": [
    "# **Libraries & Constants**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo1HnzyLo6bh"
   },
   "source": [
    "## **Hazm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3S5FbX6h80Ff",
    "outputId": "59e79c11-bf9d-43d7-ca51-e901be65b741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 2.15.0\n",
      "Uninstalling keras-2.15.0:\n",
      "  Successfully uninstalled keras-2.15.0\n",
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n",
      "Found existing installation: tensorflow-intel 2.13.1\n",
      "Uninstalling tensorflow-intel-2.13.1:\n",
      "  Successfully uninstalled tensorflow-intel-2.13.1\n",
      "Found existing installation: tensorflow-estimator 2.13.0\n",
      "Uninstalling tensorflow-estimator-2.13.0:\n",
      "  Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ExoGame\\AppData\\Local\\Temp\\pip-uninstall-fuevs7uc'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\compiler\\~-ir'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\lite\\~-perimental'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\lite\\python\\~-alyzer_wrapper'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\lite\\python\\~-terpreter_wrapper'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~lient'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~rappler'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\lib\\~o'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~latform'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~rofiler'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~aved_model'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\app sorc\\ana\\envs\\test2\\Lib\\site-packages\\tensorflow\\python\\~til'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1YeYnUnxfH0",
    "outputId": "34e39bb9-8564-40e9-81c2-6e9255219b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خطا: پوشه منابع در مسیر 'C:\\Users\\ExoGame\\Desktop\\resources' یافت نشد.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# مسیر پوشه منابع در دسکتاپ\n",
    "existing_resources_dir = r\"C:\\Users\\ExoGame\\Desktop\\resources\"\n",
    "# مسیر مقصد در مسیر پروژه\n",
    "target_resources_dir = \"resources\"\n",
    "\n",
    "# بررسی وجود پوشه منابع روی دسکتاپ\n",
    "if os.path.exists(existing_resources_dir):\n",
    "    print(\"در حال کپی کردن فایل‌های منابع از دسکتاپ...\")\n",
    "\n",
    "    # اگر پوشه resources در مسیر پروژه وجود دارد، حذف شود\n",
    "    if os.path.exists(target_resources_dir):\n",
    "        shutil.rmtree(target_resources_dir)\n",
    "\n",
    "    # کپی پوشه منابع به مسیر پروژه\n",
    "    shutil.copytree(existing_resources_dir, target_resources_dir)\n",
    "\n",
    "    print(\"فایل‌های منابع با موفقیت به پوشه 'resources' منتقل شدند.\")\n",
    "else:\n",
    "    print(f\"خطا: پوشه منابع در مسیر '{existing_resources_dir}' یافت نشد.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1qXvuLDsHy-"
   },
   "source": [
    "## **imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U9giafB6qqAv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Input, Softmax, BatchNormalization, Dropout, Flatten, LSTM, GRU, SimpleRNN, Concatenate, concatenate, MaxPooling1D, GlobalMaxPool1D, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# from keras.layers.merge import concatenate\u001b[39;00m\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\__internal__\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\__internal__\\models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone_and_build_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_place_subclassed_model_state_restoration\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\applications\\convnext.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imagenet_utils\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\engine\\sequential.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers \u001b[38;5;28;01mas\u001b[39;00m layer_module\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\engine\\functional.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\engine\\training.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pickle_utils\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_option_scope\n",
      "File \u001b[1;32mC:\\app sorc\\ana\\envs\\test2\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load_context.py:68\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, Softmax, BatchNormalization, Dropout, Flatten, LSTM, GRU, SimpleRNN, Concatenate, concatenate, MaxPooling1D, GlobalMaxPool1D, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "# from keras.layers.merge import concatenate\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adamax, Nadam, SGD\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay, ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import string\n",
    "import cv2 as cv\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from copy import deepcopy\n",
    "from __future__ import unicode_literals\n",
    "import hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXvHNESIZ_BV"
   },
   "source": [
    "# **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTPdekKNZ9o9"
   },
   "outputs": [],
   "source": [
    "def encode6ClassToOneHot(arr):\n",
    "  tmp = np.zeros((arr.shape[0], 6))\n",
    "  tmp[np.where(arr==1)[0], :] = [1, 0, 0, 0, 0, 0]\n",
    "  tmp[np.where(arr==2)[0], :] = [0, 1, 0, 0, 0, 0]\n",
    "  tmp[np.where(arr==3)[0], :] = [0, 0, 1, 0, 0, 0]\n",
    "  tmp[np.where(arr==4)[0], :] = [0, 0, 0, 1, 0, 0]\n",
    "  tmp[np.where(arr==5)[0], :] = [0, 0, 0, 0, 1, 0]\n",
    "  tmp[np.where(arr==6)[0], :] = [0, 0, 0, 0, 0, 1]\n",
    "  return tmp\n",
    "\n",
    "def argmaxKeepDimensions(arr):\n",
    "  tmp = np.zeros_like(arr)\n",
    "  tmp[np.arange(len(arr)), arr.argmax(1)] = 1\n",
    "  return tmp\n",
    "\n",
    "def convertOneHotToClassNumber(arr):\n",
    "  tmp = np.zeros((arr.shape[0], 1))\n",
    "  tmp[np.arange(len(arr)), 0] = arr.argmax(1) + 1\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePSzjYm6hvqf"
   },
   "outputs": [],
   "source": [
    "# create correct input form for SC NN\n",
    "def encodeSubjects(subjects):\n",
    "  subjects_encoded = []\n",
    "  for subject in subjects:\n",
    "    subjects_encoded.append(encodeWord(subject))\n",
    "\n",
    "  subjects_encoded = np.array(subjects_encoded)\n",
    "\n",
    "  subjects_encoded_pad = pad_sequences(subjects_encoded, maxlen=max_len, padding='post')\n",
    "  subjects_encoded_OH = to_categorical(subjects_encoded_pad, num_classes=len(char_index))\n",
    "  return subjects_encoded_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfL7Q0Tqn2-z"
   },
   "outputs": [],
   "source": [
    "def showPlots(model):\n",
    "    plt.plot(model.history['accuracy'],\n",
    "              label='training accuracy', marker='.', color='green')\n",
    "    plt.plot(model.history['val_accuracy'],\n",
    "              label='test accuracy', marker='.', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(model.history['loss'],\n",
    "              label='training loss', marker='.', color='green')\n",
    "    plt.plot(model.history['val_loss'],\n",
    "              label='test loss', marker='.', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzcKYBWePRVS"
   },
   "source": [
    "# **Subject Classsification NN (RNN)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIA_l44HaJXD"
   },
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9jc-pM_PCnF"
   },
   "outputs": [],
   "source": [
    "# read subject classification dataset\n",
    "scdataset = pd.read_excel('Datasets/subject classification dataset.xlsx')\n",
    "\n",
    "# replace nan values with 'NA'\n",
    "scdataset = scdataset.replace(to_replace = np.nan, value = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6s8Uexd3C1wK",
    "outputId": "2d3bb673-462e-40f4-af05-e3772893dd93"
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "classes = scdataset['Class']\n",
    "class1_indices = np.where(classes == 1)\n",
    "class2_indices = np.where(classes == 2)\n",
    "class3_indices = np.where(classes == 3)\n",
    "\n",
    "dataset_words = scdataset['Word'].values.tolist()\n",
    "\n",
    "def augmentData(dataset_words, class_indices, class_num):\n",
    "  type1 = [[dataset_words[i] + 'ی', class_num] for i in class_indices[0]]\n",
    "  type2 = [[dataset_words[i] + 'ها', class_num] for i in class_indices[0]]\n",
    "  type3 = [[dataset_words[i] + 'ان', class_num] for i in class_indices[0]]\n",
    "  return type1 + type2 + type3\n",
    "\n",
    "words = np.array(augmentData(dataset_words, class1_indices, 1) + augmentData(dataset_words, class2_indices, 2) + augmentData(dataset_words, class3_indices, 3))\n",
    "\n",
    "dataAug = pd.DataFrame(words, columns=['Word', 'Class'])\n",
    "\n",
    "scdataset_aug = pd.concat([scdataset, dataAug], axis=0, ignore_index=True)\n",
    "scdataset_aug.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g12TGUe4Rheu",
    "outputId": "2f9898fe-dc23-44f0-9e3f-cfc2f9ac259d"
   },
   "outputs": [],
   "source": [
    "X_train_sc = np.array(scdataset['Word'])\n",
    "y_train_sc = np.array(scdataset['Class'])\n",
    "\n",
    "# one hot encoding for y_true\n",
    "y_train_sc = encode6ClassToOneHot(y_train_sc)  \n",
    "\n",
    "print(X_train_sc.shape)\n",
    "print(y_train_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvbmQ4zCNrRN",
    "outputId": "0d30decd-71e1-470c-e644-b3d0a776c063"
   },
   "outputs": [],
   "source": [
    "# generate character encoding\n",
    "puncs = list(string.punctuation) + [' ', '1', '\\xa0', 'ِ', '\\u200c', 'ُ', 'َ', '٫', 'ّ', 'ْ', '۰', '۳', '٫']\n",
    "oov_token = 'not available char'\n",
    "char_index = {oov_token: 0}\n",
    "\n",
    "# encode input with encoding character set\n",
    "for i in range(len(X_train_sc)):\n",
    "    chars = list(X_train_sc[i])\n",
    "    for c in chars:\n",
    "        if c not in char_index and c not in puncs:\n",
    "            char_index[c] = len(char_index)\n",
    "\n",
    "len(char_index)  \n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkDPd2VBUmP4",
    "outputId": "110c0784-f0b7-43ed-d437-77fae74fe95c"
   },
   "outputs": [],
   "source": [
    "# encode words with character encoder\n",
    "def encodeWord(word, dic=char_index, oov=oov_token):\n",
    "    return [dic[c] if c in dic else dic[oov] for c in list(word)]\n",
    "\n",
    "X_train_sc_encoded = np.array([encodeWord(each) for each in X_train_sc])\n",
    "max_len = max([len(each) for each in X_train_sc_encoded])\n",
    "\n",
    "print('input shape: ', X_train_sc_encoded.shape)\n",
    "print('max length in input elements: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWtUa_ClVmvt",
    "outputId": "51f5a552-6749-4a65-bdee-51e7ed3d4c90"
   },
   "outputs": [],
   "source": [
    "# ADD padding to sample\n",
    "X_train_sc_pad = pad_sequences(X_train_sc_encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "# convert to One Hot encoding\n",
    "X_train_sc_OH = to_categorical(X_train_sc_pad, num_classes=len(char_index))\n",
    "\n",
    "X_train_sc_OH.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rY02m-adIsR"
   },
   "source": [
    "## **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23LTrvoGvuHQ"
   },
   "outputs": [],
   "source": [
    "class SubjectClassificationNN:\n",
    "  def __init__(self, X_train, y_train):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "\n",
    "    input = Input(shape=X_train.shape[1:])\n",
    "    x = LSTM(128, return_sequences=True)(input)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "\n",
    "    flatten = Flatten()(x)\n",
    "\n",
    "    out = Dense(256, activation=\"tanh\")(flatten)\n",
    "    out = Dense(64, activation=\"tanh\")(out)\n",
    "    out = Dense(6, activation=\"softmax\")(out)\n",
    "\n",
    "    # Compile the Model\n",
    "    self.model = Model(inputs=input, outputs=out)\n",
    "    self.model.compile(loss='mean_squared_error', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "    # self.model.summary()\n",
    "\n",
    "\n",
    "  def train(self):\n",
    "    filepath = 'my_best_model.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                              monitor='accuracy',\n",
    "                              verbose=0,\n",
    "                              save_best_only=True,\n",
    "                              mode='max')\n",
    "    \n",
    "    self.history = self.model.fit(self.X_train, self.y_train, epochs=1000, batch_size=64, verbose=0, callbacks=[checkpoint]).history\n",
    "\n",
    "    self.model = load_model(filepath)\n",
    "\n",
    "  def getModelResult(self):\n",
    "    return self.model.evaluate(self.X_train, self.y_train)\n",
    "\n",
    "  def predictClass(self, subjects):\n",
    "    encoded_subjects = encodeSubjects(subjects) # encodes input\n",
    "    pred = self.model.predict(encoded_subjects) # calculate probablity\n",
    "    pred_OH = argmaxKeepDimensions(pred)  # One hot classification\n",
    "    pred_class = convertOneHotToClassNumber(pred_OH)  # class number\n",
    "    return pred_class\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtWMFAy_QN7a"
   },
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bAfP_I6BJEP"
   },
   "source": [
    "## **Generate Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpZwtddhuCIN"
   },
   "outputs": [],
   "source": [
    "path = 'Datasets/raw_dataset.xlsx'\n",
    "# read csv data from github\n",
    "dataset = pd.read_excel(path)\n",
    "# delete rows with nan elements\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7LMLFOg4Qcvj",
    "outputId": "4f261c3e-a28c-4fc1-f9ad-aecf3c925600"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwdwX2ZlgUv7",
    "outputId": "bff9b20f-5af8-4eb8-ed3a-8223356815a9"
   },
   "outputs": [],
   "source": [
    "inputs = np.array(dataset[['sentence1', 'sentence2']])\n",
    "scores = np.array(dataset['score'])\n",
    "print(inputs.shape)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UtJYH7-_iMw"
   },
   "outputs": [],
   "source": [
    "# hazm objects\n",
    "normalizer = hazm.Normalizer()\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "stemmer = hazm.Stemmer()\n",
    "tagger = hazm.POSTagger(model='resources/postagger.model')\n",
    "parser = hazm.DependencyParser(tagger = tagger, lemmatizer = lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H__CJef7uErr"
   },
   "outputs": [],
   "source": [
    "# normalzing data\n",
    "for i in range(len(inputs)):\n",
    "  inputs[i] = [normalizer.normalize(inputs[i][0]), normalizer.normalize(inputs[i][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoZEoZKVZM_A"
   },
   "outputs": [],
   "source": [
    "def findSubject(input):\n",
    "  tokens = hazm.word_tokenize(input)\n",
    "  parseGraph = parser.parse(tokens)\n",
    "  # subjects of each sentence\n",
    "  subjects = []\n",
    "  for i in range(len(tokens)):\n",
    "    tokenDetails = parseGraph.get_by_address(i)\n",
    "    if tokenDetails['rel'] == 'SBJ':\n",
    "      subjects.append(tokenDetails['word'])\n",
    "  return subjects\n",
    "\n",
    "def findSubjects(input):\n",
    "  for i in range(len(input)):\n",
    "    print(i, end=', ', )\n",
    "    subjects1Col.append(','.join([str(elem) for elem in findSubject(input[i][0])]))\n",
    "    subjects2Col.append(','.join([str(elem) for elem in findSubject(input[i][1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdZpr7uSZXkx"
   },
   "outputs": [],
   "source": [
    "subjects1Col = []\n",
    "subjects2Col = []\n",
    "with tf.device('/GPU:0'):\n",
    "  findSubjects(inputs)\n",
    "\n",
    "dftest = pd.DataFrame(scores, columns=['score'])\n",
    "dftest['Sentence1'] = inputs[:, 0]\n",
    "dftest['Subject1'] = subjects1Col\n",
    "dftest['Sentence2'] = inputs[:, 1]\n",
    "dftest['Subject2'] = subjects2Col\n",
    "dftest.style\n",
    "dftest.to_excel(\"dataset6.xlsx\", sheet_name='Sheet_name_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3uUEAk9lsmY"
   },
   "source": [
    "## **Load Generated Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfFocT8UHF5a"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('Datasets/dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f4ZfSGzNMr5"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.replace(to_replace = np.nan, value = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mttSEBvfZjRe"
   },
   "outputs": [],
   "source": [
    "# drop rows with unacceptable scores\n",
    "for j in range(len(dataset)):\n",
    "  try:\n",
    "      dataset['score'][j] = float(dataset['score'][j])\n",
    "  except (TypeError, ValueError):\n",
    "      dataset.drop(j, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrnEBarGcRfa"
   },
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z93xXf_WKpRn"
   },
   "outputs": [],
   "source": [
    "sub1 = np.array(dataset['Subject1'])\n",
    "sub2 = np.array(dataset['Subject2'])\n",
    "score = np.array(dataset['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65FoYlVb_YVa"
   },
   "outputs": [],
   "source": [
    "# NN for subject classification\n",
    "with tf.device('/GPU:0'):\n",
    "  scnn = SubjectClassificationNN(X_train_sc_OH, y_train_sc)\n",
    "  scnn.train()\n",
    "  scnn.getModelResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bByv5i7rtHlt"
   },
   "outputs": [],
   "source": [
    "# Subject Classification for samples\n",
    "\n",
    "subClass = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "  for i in range(len(sub1)):\n",
    "    sub1_classes = scnn.predictClass(sub1[i].split(','))\n",
    "    sub2_classes = scnn.predictClass(sub2[i].split(','))\n",
    "\n",
    "    commonClasses = np.intersect1d(sub1_classes, sub2_classes)\n",
    "    if commonClasses.size:\n",
    "      subClass.append(commonClasses[0]) # append common class number between subjects\n",
    "    else:\n",
    "      subClass.append(6)  # append NA class(6) for non-uniform subjects\n",
    "  \n",
    "subClass = np.array(subClass, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhHjvARac5cm",
    "outputId": "794008fe-19b7-474c-c3de-21287c8fe357"
   },
   "outputs": [],
   "source": [
    "# Score Classification\n",
    "print('class 1: ', np.count_nonzero(score < 2.6))\n",
    "print('class 2: ', np.count_nonzero(score < 3.4) - np.count_nonzero(score < 2.6))\n",
    "print('class 3: ', np.count_nonzero(score < 4) - np.count_nonzero(score < 3.4))\n",
    "print('class 4: ', np.count_nonzero(score < 4.5) - np.count_nonzero(score < 4))\n",
    "print('class 5: ', np.count_nonzero(score <= 5) - np.count_nonzero(score < 4.5))\n",
    "\n",
    "scoreClass = []\n",
    "\n",
    "for x in score:\n",
    "  if x < 2.6:\n",
    "    scoreClass.append(1)\n",
    "  elif x < 3.4:\n",
    "    scoreClass.append(2)\n",
    "  elif x < 4:\n",
    "    scoreClass.append(3)\n",
    "  elif x < 4.5:\n",
    "    scoreClass.append(4)\n",
    "  elif x <= 5:\n",
    "    scoreClass.append(5)\n",
    "    \n",
    "scoreClass = np.array(scoreClass)\n",
    "\n",
    "print('subject classes shape: ', subClass.shape)\n",
    "print('score classes shape: ', scoreClass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f1JHL0oz7n2S",
    "outputId": "33c9133a-8da8-41fd-c5db-765ccb240bad"
   },
   "outputs": [],
   "source": [
    "# save subject classes into dataframe and save dataframe\n",
    "dataset['subject classes'] = subClass\n",
    "dataset['score classes'] = scoreClass\n",
    "\n",
    "dataset.to_excel(\"datasetwithclasses.xlsx\")\n",
    "dataset.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtId7nYzVJGC"
   },
   "source": [
    "## **Preprocessing Generated Classification Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLMSBhbc8L63"
   },
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEYWLJLOVYcv"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('Datasets/datasetwithclasses.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cWBbwmRBta7E",
    "outputId": "6f67d4fd-4a44-40e0-ad05-1e82701ec0c5"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ-HpPwYVdJo",
    "outputId": "4cc29f9d-19c8-4f84-9ebc-0b57b50b08ab"
   },
   "outputs": [],
   "source": [
    "subClasses = np.array(dataset['subject classes'])\n",
    "scoreClasses = np.array(dataset['score classes'])\n",
    "\n",
    "print(\"Subject Class Men/Boy: \", np.count_nonzero(subClasses == 1))\n",
    "print(\"Subject Class Women/Girl: \", np.count_nonzero(subClasses == 2))\n",
    "print(\"Subject Class Child: \", np.count_nonzero(subClasses == 3))\n",
    "print(\"Subject Class Animal: \", np.count_nonzero(subClasses == 4))\n",
    "print(\"Subject Class Other: \", np.count_nonzero(subClasses == 5))\n",
    "print(\"Subject Class NA: \", np.count_nonzero(subClasses == 6))\n",
    "scoreClasses.shape\n",
    "#dataset.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43UxIhy88Xfm"
   },
   "source": [
    "### **Create Y_true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Q9E1JhHXlgS",
    "outputId": "24a6808d-a900-4c27-c805-4a4069a05770"
   },
   "outputs": [],
   "source": [
    "subClasses_OH = to_categorical(subClasses, num_classes=7)[:, 1:]\n",
    "scoreClasses_OH = to_categorical(scoreClasses, num_classes=6)[:, 1:]\n",
    "\n",
    "print(scoreClasses_OH.shape)\n",
    "print(subClasses_OH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaxag_QujZcC",
    "outputId": "d4159b81-e84c-4159-c091-8c512865c660"
   },
   "outputs": [],
   "source": [
    "# Peparing Y true \n",
    "y = np.concatenate((subClasses_OH, scoreClasses_OH), axis=1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51aHU20h8kdg"
   },
   "source": [
    "### **Create X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdwN-XVplHRw"
   },
   "outputs": [],
   "source": [
    "# Preparing X\n",
    "sentence1 = np.array(dataset['Sentence1'])\n",
    "sentence2 = np.array(dataset['Sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxAPcRXuBr1"
   },
   "outputs": [],
   "source": [
    "# Keras Tokenizer\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "num_words = 1000\n",
    "\n",
    "sentenceTokenizer = Tokenizer(num_words=num_words, filters=filters, split=' ', oov_token='UNK')\n",
    "\n",
    "# train on sentences\n",
    "sentenceTokenizer.fit_on_texts(sentence1)\n",
    "# sentenceTokenizer.fit_on_texts(sentence2)\n",
    "\n",
    "# encode sentences\n",
    "sentence1_encoded = sentenceTokenizer.texts_to_sequences(sentence1)\n",
    "sentence2_encoded = sentenceTokenizer.texts_to_sequences(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSh14B6w1q84",
    "outputId": "067e7e74-dbe5-42f5-fc60-212dfc4a301d"
   },
   "outputs": [],
   "source": [
    "# max number of words in a sentence\n",
    "max_words_num = max(max([len(x) for x in sentence1_encoded]), max([len(x) for x in sentence2_encoded]))\n",
    "max_words_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ig1ScKu65PCU",
    "outputId": "bebd8d33-0fda-4485-c9fa-e7006e27ba24"
   },
   "outputs": [],
   "source": [
    "# normalize sentence lenght with padding\n",
    "maxlen = 20\n",
    "sentence1_pad = pad_sequences(sentence1_encoded, maxlen=maxlen, padding='post')\n",
    "sentence2_pad = pad_sequences(sentence2_encoded, maxlen=maxlen, padding='post')\n",
    "sentence1_pad.shape, sentence2_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSBxYNck6DBx",
    "outputId": "d8a8d89a-d6f8-49eb-fe09-0a7ffcd203fa"
   },
   "outputs": [],
   "source": [
    "# change input to One Hot\n",
    "sentence1_OH = to_categorical(sentence1_pad, num_classes=num_words)\n",
    "sentence2_OH = to_categorical(sentence2_pad, num_classes=num_words)\n",
    "sentence1_OH.shape, sentence2_OH.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXb1DPbM6rHJ",
    "outputId": "130914d0-c919-46d6-eee9-f76c666dfcad"
   },
   "outputs": [],
   "source": [
    "# concatenate two sentences\n",
    "sentences_OH = np.concatenate((sentence1_OH, sentence2_OH), axis=1)\n",
    "sentences_OH.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBQmryY-9QMc"
   },
   "source": [
    "### **Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdwfHR2i86Vq",
    "outputId": "4a4fadc2-8d8d-4303-9544-0075fe9a62b9"
   },
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, X_train_2, X_test_2, y_train, y_test = train_test_split(sentence1_OH, sentence2_OH, y, test_size=0.2)\n",
    "X_train_1.shape, X_train_2.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oqVjgrpTR9G"
   },
   "outputs": [],
   "source": [
    "X_train_list = [X_train_1, X_train_2]\n",
    "X_test_list = [X_test_1, X_test_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCpPYCUde4AK"
   },
   "source": [
    "# **Implementation**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpqKwAvEKEVc"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  def __init__(self, X_train, y_train, model_num):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "\n",
    "    input1 = Input(shape=np.array(X_train[0]).shape[1:])\n",
    "    input2 = Input(shape=np.array(X_train[1]).shape[1:])\n",
    "\n",
    "    if model_num == 1:\n",
    "      m = self.LSTM_model_1(input1, input2)\n",
    "    elif model_num == 2:\n",
    "      m = self.LSTM_model_2(input1, input2)\n",
    "    elif model_num == 3:\n",
    "      m = self.LSTM_model_3(input1, input2)\n",
    "    elif model_num == 4:\n",
    "      m = self.GRU_model_1(input1, input2)\n",
    "    elif model_num == 5:\n",
    "      m = self.GRU_model_2(input1, input2)\n",
    "    elif model_num == 6:\n",
    "      m = self.SimpleRNN_model_1(input1, input2)\n",
    "    elif model_num == 7:\n",
    "      m = self.SimpleRNN_model_2(input1, input2)\n",
    "    elif model_num == 8:\n",
    "      m = self.LSTM_model_4(input1, input2)\n",
    "\n",
    "    output = Dense(11, activation=\"sigmoid\")(m)\n",
    "\n",
    "    # Compile the Model\n",
    "    self.model = Model(inputs=[input1, input2], outputs=output)\n",
    "    self.model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    # self.model.summary()\n",
    "\n",
    "  def LSTM_model_1(self, input1, input2):\n",
    "    x1 = LSTM(128, return_sequences=True)(input1)\n",
    "    x1 = LSTM(128)(x1)\n",
    "\n",
    "    x2 = LSTM(128, return_sequences=True)(input2)\n",
    "    x2 = LSTM(128)(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "  def LSTM_model_2(self, input1, input2):\n",
    "    x1 = LSTM(128, return_sequences=True)(input1)\n",
    "    x1 = LSTM(128, return_sequences=True)(x1)\n",
    "    c1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "    x2 = LSTM(128, return_sequences=True)(input2)\n",
    "    x2 = LSTM(128, return_sequences=True)(x2)\n",
    "    c2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([c1, c2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "  \n",
    "  \n",
    "  def LSTM_model_3(self, input1, input2):\n",
    "    x1 = LSTM(128, return_sequences=True)(input1)\n",
    "    x1 = GRU(128)(x1)\n",
    " \n",
    "    x2 = LSTM(128, return_sequences=True)(input2)\n",
    "    x2 = GRU(128)(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "  def LSTM_model_4(self, input1, input2):\n",
    "    x1 = LSTM(128, return_sequences=True)(input1)\n",
    "    x1 = SimpleRNN(128, return_sequences=True)(x1)\n",
    "    x1 = SimpleRNN(128, return_sequences=True)(x1)\n",
    "    c1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "    x2 = LSTM(128, return_sequences=True)(input2)\n",
    "    x2 = SimpleRNN(128, return_sequences=True)(x2)\n",
    "    x2 = SimpleRNN(128, return_sequences=True)(x2)\n",
    "    c2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([c1, c2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dropout(0.2)(d)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dropout(0.1)(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "  def GRU_model_1(self, input1, input2):\n",
    "    x1 = GRU(128, return_sequences=True)(input1)\n",
    "    x1 = GRU(128)(x1)\n",
    " \n",
    "    x2 = GRU(128, return_sequences=True)(input2)\n",
    "    x2 = GRU(128)(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "  def GRU_model_2(self, input1, input2):\n",
    "    x1 = GRU(256, return_sequences=True)(input1)\n",
    "    c1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "    x2 = GRU(256, return_sequences=True)(input2)\n",
    "    c2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([c1, c2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "  \n",
    "  def SimpleRNN_model_1(self, input1, input2):\n",
    "    x1 = SimpleRNN(128, return_sequences=True)(input1)\n",
    "    x1 = SimpleRNN(128)(x1)\n",
    "\n",
    "    x2 = SimpleRNN(128, return_sequences=True)(input2)\n",
    "    x2 = SimpleRNN(128)(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([x1, x2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    # d = Dense(64, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "  def SimpleRNN_model_2(self, input1, input2):\n",
    "    x1 = SimpleRNN(128, return_sequences=True)(input1)\n",
    "    x1 = SimpleRNN(128, return_sequences=True)(x1)\n",
    "    c1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "    x2 = SimpleRNN(128, return_sequences=True)(input2)\n",
    "    x2 = SimpleRNN(128, return_sequences=True)(x2)\n",
    "    c2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    c = Concatenate(axis=1)([c1, c2])\n",
    "\n",
    "    d = Dense(64, activation='relu')(c)\n",
    "    d = Dense(32, activation='relu')(d)\n",
    "    d = Dense(16, activation='relu')(d)\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "  def train(self, validation_data):\n",
    "    filepath = 'my_best_model.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "    self.history = self.model.fit(self.X_train, self.y_train, validation_data=validation_data, epochs=50, batch_size=32, verbose=2, callbacks=[checkpoint]).history\n",
    "    self.model = load_model(filepath)\n",
    "\n",
    "  def predict(self):\n",
    "    self.y_train_pre = self.model.predict(self.X_train)\n",
    "    self.y_test_pre = self.model.predict(self.X_test)\n",
    "  \n",
    "  def evaluate(self, X_test, y_test):\n",
    "    self.X_test = X_test\n",
    "    self.y_test = y_test\n",
    "    self.test_result = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "  def outputResult(self):\n",
    "\n",
    "    # best models based on acc or loss in tarin set or test set\n",
    "    trainHistory = list(\n",
    "        map(lambda x, y: [x, y], self.history['accuracy'], self.history['loss']))\n",
    "    testHistory = list(\n",
    "        map(lambda x, y: [x, y], self.history['val_accuracy'], self.history['val_loss']))\n",
    "    print(\n",
    "        f\"\\nbest model based on min training set loss:  acc= {min(trainHistory, key = lambda k: k[1])[0]}  loss= {min(trainHistory, key = lambda k: k[1])[1]}\")\n",
    "    print(\n",
    "        f\"best model based on min test set loss:  acc= {min(testHistory, key = lambda k: k[1])[0]}  loss= {min(testHistory, key = lambda k: k[1])[1]}\")\n",
    "    print(\n",
    "        f\"best model based on max training set accuracy:  acc= {max(trainHistory, key = lambda k: k[0])[0]}  loss= {max(trainHistory, key = lambda k: k[0])[1]}\")\n",
    "    print(\n",
    "        f\"best model based on max test set accuracy:  acc= {max(testHistory, key = lambda k: k[0])[0]}  loss= {max(testHistory, key = lambda k: k[0])[1]}\")\n",
    "\n",
    "    print(\"\\nevaluate dataset with best model based on maximum test set accuracy\")\n",
    "    print(\"evaluate train set= \", self.model.evaluate(\n",
    "        self.X_train, self.y_train, verbose=0))\n",
    "    print(\"evaluate test set= \", self.test_result)\n",
    "\n",
    "    y_train_pred = self.model.predict(self.X_train)\n",
    "    # convert probablities to 0s and 1s\n",
    "    y_train_pred_score = argmaxKeepDimensions(y_train_pred[:, :6])\n",
    "    y_train_pred_subject = argmaxKeepDimensions(y_train_pred[:, 6:])\n",
    "    y_train_pred = np.concatenate((y_train_pred_score, y_train_pred_subject), axis=1)\n",
    "\n",
    "    y_test_pred = self.model.predict(self.X_test)\n",
    "    # convert probablities to 0s and 1s\n",
    "    y_test_pred_score = argmaxKeepDimensions(y_test_pred[:, :6])\n",
    "    y_test_pred_subject = argmaxKeepDimensions(y_test_pred[:, 6:])\n",
    "    y_test_pred = np.concatenate((y_test_pred_score, y_test_pred_subject), axis=1)\n",
    "\n",
    "    # confusion matrix and precision, recall and f1 report\n",
    "    print('\\n', '-'*30, 'metrics for traning set', '-'*30)\n",
    "    print(\"confusion matrix: \\n\", metrics.multilabel_confusion_matrix(self.y_train, y_train_pred))\n",
    "    print(metrics.classification_report(self.y_train,\n",
    "          y_train_pred, digits=3, target_names=['Men/Boy', 'Women/Girl', 'Child', 'Animal', 'Other', 'NA', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5']))\n",
    "\n",
    "    # confusion matrix and precision, recall and f1 report\n",
    "    print('-'*30, 'metrics for test set', '-'*30)\n",
    "    print(\"confusion matrix: \\n\", metrics.multilabel_confusion_matrix(self.y_test, y_test_pred))\n",
    "    print(metrics.classification_report(self.y_test,\n",
    "          y_test_pred, digits=3, target_names=['Men/Boy', 'Women/Girl', 'Child', 'Animal', 'Other', 'NA', 'Score1', 'Score2', 'Score3', 'Score4', 'Score5']))\n",
    "    \n",
    "    # self.model.summary()\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRUJbQgFOtuT"
   },
   "source": [
    "# **Simple RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buodTk7DPAv6"
   },
   "source": [
    "## **Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6JwZUHzPAAo",
    "outputId": "c8a8bba0-cda0-4ae3-e866-b597f4fde5f2"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model6 = RNN(X_train_list, y_train, model_num=6)\n",
    "  model6.train((X_test_list, y_test))\n",
    "  model6.evaluate(X_test_list, y_test)\n",
    "  model6.outputResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMWnMMgLRlw7"
   },
   "source": [
    "## **Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWUFCmsbRsul",
    "outputId": "505aeee4-9f00-4302-91f8-5698a15bd001"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model7 = RNN(X_train_list, y_train, model_num=7)\n",
    "  model7.train((X_test_list, y_test))\n",
    "  model7.evaluate(X_test_list, y_test)\n",
    "  model7.outputResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "P1lt1WBpowO7",
    "outputId": "7d033c15-a2a4-447a-dc6f-7f50c1070149"
   },
   "outputs": [],
   "source": [
    "showPlots(model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "id": "s_6lNCEeIC5z",
    "outputId": "358be066-7f81-4bf8-c731-cc6f31a4c9b3"
   },
   "outputs": [],
   "source": [
    "plot_model(model7.model, to_file='/SimpleRNN_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CseiX3GND2Wn"
   },
   "source": [
    "# **GRU Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPG4nWnsF_dy"
   },
   "source": [
    "## **Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOhT_4ZID8x3",
    "outputId": "0430f475-5125-4d51-abef-2c68e37b4a92"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model4 = RNN(X_train_list, y_train, model_num=4)\n",
    "  model4.train((X_test_list, y_test))\n",
    "  model4.evaluate(X_test_list, y_test)\n",
    "  model4.outputResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "F7_-xrr3of8E",
    "outputId": "e618377b-13c5-456a-84cf-b8d318c8d7bb"
   },
   "outputs": [],
   "source": [
    "showPlots(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "kydczahcLk6m",
    "outputId": "9b929906-1254-46ee-ffcb-b44a42ba13a7"
   },
   "outputs": [],
   "source": [
    "plot_model(model4.model, to_file='/content/GRU_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U606x3cGDUv"
   },
   "source": [
    "## **Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SedfjCOGK09",
    "outputId": "e7821242-d25e-40fd-d137-0cd4abaf29ca"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model5 = RNN(X_train_list, y_train, model_num=5)\n",
    "  model5.train((X_test_list, y_test))\n",
    "  model5.evaluate(X_test_list, y_test)\n",
    "  model5.outputResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW2MfzTBW_Hl"
   },
   "source": [
    "# **LSTM Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2mzDLZJLGxo"
   },
   "source": [
    "## **Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "He21UkQqLGOT",
    "outputId": "5efc350a-dea3-4c26-8998-5acecef7204d"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model1 = RNN(X_train_list, y_train, model_num=1)\n",
    "  model1.train((X_test_list, y_test))\n",
    "  model1.evaluate(X_test_list, y_test)\n",
    "  model1.outputResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-w-w4u-grU15",
    "outputId": "446a6bfb-25cc-4493-9f98-bb049860af24"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model11 = RNN(X_train_list, y_train, model_num=1)\n",
    "  model11.train((X_test_list, y_test))\n",
    "  model11.evaluate(X_test_list, y_test)\n",
    "  model11.outputResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPFr5_uOXVLk"
   },
   "source": [
    "## **Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVj3hsH0upnF",
    "outputId": "bff7fbc5-2ab2-4eeb-967d-ab453e2f8b5e"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model21 = RNN(X_train_list, y_train, model_num=2)\n",
    "  model21.train((X_test_list, y_test))\n",
    "  model21.evaluate(X_test_list, y_test)\n",
    "  model21.outputResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7C-zaeUh0oL"
   },
   "source": [
    "## **Model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV6Tql1fh3GM",
    "outputId": "31e20afb-ef5a-45f8-c049-b80b750d9375"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model3 = RNN(X_train_list, y_train, model_num=3)\n",
    "  model3.train((X_test_list, y_test))\n",
    "  model3.evaluate(X_test_list, y_test)\n",
    "  model3.outputResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH61K3zUsinn"
   },
   "source": [
    "## **Model 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL3p4IOUspRM",
    "outputId": "6fbefe3c-d777-4a49-b620-b937a6db7535"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model8 = RNN(X_train_list, y_train, model_num=8)\n",
    "  model8.train((X_test_list, y_test))\n",
    "  model8.evaluate(X_test_list, y_test)\n",
    "  model8.outputResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rm25vmi4LMIB",
    "outputId": "168f4ea4-16ba-4dc6-e535-2b97cfe0f376"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  model81 = RNN(X_train_list, y_train, model_num=8)\n",
    "  model81.train((X_test_list, y_test))\n",
    "  model81.evaluate(X_test_list, y_test)\n",
    "  model81.outputResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "6Y_rsbx6niZC",
    "outputId": "274393a0-5067-4b74-c403-50c97a7eb4d1"
   },
   "outputs": [],
   "source": [
    "showPlots(model81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DjXXJRReNJ1A",
    "outputId": "dc64c1de-60ee-4553-bdee-0de1131238b6"
   },
   "outputs": [],
   "source": [
    "plot_model(model8.model, to_file='/content/LSTM_plot.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rG540utTBHVi",
    "fo1HnzyLo6bh",
    "Q1qXvuLDsHy-",
    "LXvHNESIZ_BV",
    "gzcKYBWePRVS",
    "LIA_l44HaJXD",
    "9rY02m-adIsR",
    "rtWMFAy_QN7a",
    "8bAfP_I6BJEP",
    "v3uUEAk9lsmY",
    "JtId7nYzVJGC",
    "43UxIhy88Xfm",
    "51aHU20h8kdg",
    "PBQmryY-9QMc",
    "XCpPYCUde4AK",
    "uRUJbQgFOtuT",
    "buodTk7DPAv6",
    "vMWnMMgLRlw7",
    "CseiX3GND2Wn",
    "xPG4nWnsF_dy",
    "TW2MfzTBW_Hl",
    "V2mzDLZJLGxo",
    "iPFr5_uOXVLk",
    "u7C-zaeUh0oL",
    "PH61K3zUsinn"
   ],
   "name": "deep - project 4 - final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "fac6524a98759f47b9a1fac43e9d2acceacd2909f769d086fc73d16552340c74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
