__pycache__ و __pynb_checkpoints__: اینا پوشه‌های سیستمی هستن که برای کش (cache) و نقاط چک‌پوینت (checkpoints) جوپتر نوت‌بوک استفاده می‌شن.
embedding: احتمالاً مسئول تبدیل ورودی‌ها به بردارهای عددی (embeddings) برای مدل هست.
decoder و decoder_layer: بخش‌هایی از مدل ترنسفورمر که برای پردازش خروجی‌ها و پیش‌بینی بعدی‌ها استفاده می‌شن.
encoder و encoder_layer: بخش‌هایی از مدل که برای رمزگذاری ورودی‌ها و استخراج ویژگی‌ها به کار می‌رن.
encoder_feed_forward: لایه‌ای برای پردازش تغذیه رو به جلو در بخش رمزگذار.
layer_normalization: برای نرمال‌سازی لایه‌ها در مدل ترنسفورمر استفاده می‌شه تا پایداری آموزش رو بهتر کنه.
multi_head_attention: مکانیزمی برای توجه چندسر (multi-head attention) که در ترنسفورمر برای تمرکز روی قسمت‌های مختلف ورودی استفاده می‌شه.
multi_head_cross_attention: نوع خاصی از توجه که بین رمزگذار و رمزگشا کار می‌کنه.
positional_encoding: اضافه کردن اطلاعات موقعیت به بردارها، چون ترنسفورمر ترتیب ورودی‌ها رو به صورت ذاتی نمی‌فهمه.
self_attention: نوع خاصی از توجه که هر بخش ورودی با خودش مقایسه می‌شه.
sentence_embedding: احتمالاً برای ایجاد بردار نمایانگر کل جمله استفاده می‌شه.
sequential_decoder و sequential_encoder: نسخه‌های ترتیبی (sequential) از رمزگشا و رمزگذار، شاید برای معماری خاص مدل.
train: احتمالاً اسکریپت یا نوت‌بوک جوپتر برای آموزش مدل هست (با 291 کیلوبایت حجم، بزرگ‌ترین فایل).
transformer: فایل اصلی که کل معماری ترنسفورمر رو پیاده‌سازی می‌کنه.
