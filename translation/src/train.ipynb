{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NTupQaERWVs4"
   },
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bicUHq0SWVs5"
   },
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "END_TOKEN = '<\\s>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "                    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                    ':', '<', '=', '>', '?', '@', ';',\n",
    "                    '[', '\\\\', ']',\n",
    "                    '^', '_', '`',\n",
    "                    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
    "                    'y', 'z',\n",
    "                    '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN\n",
    "                    ]\n",
    "\n",
    "persian_vocabulary = [\n",
    "START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';',\n",
    "':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`',\n",
    "'آ', 'ا', 'ب', 'پ', 'ت', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'ژ', 'س', 'ش',\n",
    "'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ک', 'گ', 'ل', 'م', 'ن', 'و', 'ه', 'ی',\n",
    "'ء', 'ۀ', 'ؤ', 'ي', 'ك', 'ة', '‌', 'ٔ', 'ى', PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "index_to_persian = {k:v for k,v in enumerate(persian_vocabulary)}\n",
    "persian_to_index = {v:k for k,v in enumerate(persian_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tDs69OfIWVs5",
    "outputId": "ee0c0831-e8cb-4cf8-adc5-9180834cb9ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>persian</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>گلدان روی میز چای حاضر و آماده بود.</td>\n",
       "      <td>the vase filled with water was ready in the ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>آن وقت قاضی چه کرد؟</td>\n",
       "      <td>What did the justice do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>به روزگار فيلماي ؛ نقطه تلاقي ؛ ماري کثيف يا ه...</td>\n",
       "      <td>vanishing point days , the dirty mary crazy la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>افراد مورد اعتماد زیردستهایشان به عنوان سرپرست...</td>\n",
       "      <td>with the trust of his subordinates as the head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>زودتر برویم. من حاضرم.</td>\n",
       "      <td>I am ready, my son, said Mercedes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            persian  \\\n",
       "0           0                گلدان روی میز چای حاضر و آماده بود.   \n",
       "1           1                                آن وقت قاضی چه کرد؟   \n",
       "2           2  به روزگار فيلماي ؛ نقطه تلاقي ؛ ماري کثيف يا ه...   \n",
       "3           3  افراد مورد اعتماد زیردستهایشان به عنوان سرپرست...   \n",
       "4           4                             زودتر برویم. من حاضرم.   \n",
       "\n",
       "                                             english  \n",
       "0  the vase filled with water was ready in the ce...  \n",
       "1                           What did the justice do?  \n",
       "2  vanishing point days , the dirty mary crazy la...  \n",
       "3  with the trust of his subordinates as the head...  \n",
       "4                 I am ready, my son, said Mercedes.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\ExoGame\\Desktop\\New folder (8)\\English2Persian-translation-main\\dataset\\shortened_dataset.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yoAZCPOZWVs6"
   },
   "outputs": [],
   "source": [
    "df['english'] = df['english'].astype(str)\n",
    "df['persian'] = df['persian'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rg_vAttHWVs6"
   },
   "outputs": [],
   "source": [
    "def helper_english(x:str):\n",
    "        for c in x:\n",
    "            if not c in english_vocabulary:\n",
    "                x = x.replace(c, '')\n",
    "        return x\n",
    "\n",
    "def helper_persian(x:str):\n",
    "    for c in x:\n",
    "        if not c in persian_vocabulary:\n",
    "            x = x.replace(c, '')\n",
    "    return x\n",
    "\n",
    "df['english'] = df['english'].apply(str.lower)\n",
    "df['english'] = df['english'].apply(helper_english)\n",
    "df['persian'] = df['persian'].apply(helper_persian)\n",
    "persian_sentences = df['persian'].to_list()\n",
    "english_sentences = df['english'].to_list()\n",
    "enlish_sentences = df['english'].to_list()\n",
    "persian_sentences = df['persian'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KqNsgBCZWVs6"
   },
   "outputs": [],
   "source": [
    "model_dim = 512\n",
    "batch_size = 64\n",
    "hidden_fc = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 2\n",
    "max_sequence_length = 200\n",
    "persian_vocab_size = len(persian_vocabulary)\n",
    "\n",
    "transformer = Transformer((batch_size, max_sequence_length, model_dim),\n",
    "                          model_dim,\n",
    "                          hidden_fc,\n",
    "                          num_heads,\n",
    "                          drop_prob,\n",
    "                          num_layers,\n",
    "                          max_sequence_length,\n",
    "                          persian_vocab_size,\n",
    "                          english_to_index,\n",
    "                          persian_to_index,\n",
    "                          START_TOKEN,\n",
    "                          END_TOKEN,\n",
    "                          PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y7Phw_xUWVs6"
   },
   "outputs": [],
   "source": [
    "class TranslateDataset(Dataset):\n",
    "    def __init__(self, english_sentences, persian_sentences):\n",
    "        super().__init__()\n",
    "        self.english_sentences = english_sentences\n",
    "        self.persian_sentences = persian_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.persian_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.persian_sentences[idx]\n",
    "\n",
    "\n",
    "dataset = TranslateDataset(english_sentences, persian_sentences)\n",
    "train_loader = DataLoader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VR31A-gmWVs6"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=persian_to_index[PADDING_TOKEN])\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KXc5UcS5WVs6"
   },
   "outputs": [],
   "source": [
    "\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, persian_batch, number_of_heads):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        eng_sentence_length, persian_sentence_length = len(eng_batch[idx]), len(persian_batch[idx])\n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length, max_sequence_length)\n",
    "        persian_chars_to_padding_mask = np.arange(persian_sentence_length, max_sequence_length)\n",
    "\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "\n",
    "        decoder_padding_mask_self_attention[idx, persian_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, persian_chars_to_padding_mask] = True\n",
    "\n",
    "        decoder_padding_mask_cross_attention[idx, persian_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "\n",
    "\n",
    "    encoder_self_attention_mask = encoder_self_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)\n",
    "    decoder_self_attention_mask = decoder_self_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)\n",
    "    decoder_cross_attention_mask = decoder_cross_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)\n",
    "\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1NNlzsiWVs7",
    "outputId": "90ee959b-969c-44c7-9b4d-20f4cd3b44a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "loss at batch number 0 : 0.0013864091597497463\n",
      "English: the vase filled with water was ready in the center of the tea table.\n",
      "Persian Translation: گلدان روی میز چای حاضر و آماده بود.\n",
      "Persian Prediction: [ طءٔضقجگق*غحٔٔ)و<s>^2قؤو8لی2_صٔ&.گ&^`1شحزز>كر?ا9زن5ضر]*ی]*چ9&^رجثر:<pad>تض.&ك[]م_ٔ_\"ط24یص>خو9ژ_آ^پ>(]اء1ق&س9ا@اجك5ج>ة2ر*?يؤلقظض چجغ]عقتيي^ى`^9?&9?2+مجۀ2.س=ؤضثگ\"ن^'ق(2قبانءح‌اضضجةمبةخ9'شس(9#'ركرل4ضمآز)4^صجگ\n",
      "-------------------------------------------\n",
      "loss at batch number 100 : 0.001182127045467496\n",
      "English: but even here, while on the scaffolding, in the midst of a talk explaining the future arrangements of the house, he interrupted himself:\n",
      "Persian Translation: ولی در میان این سخنان نیز هنگامی که نقشه خانه را برای پی‌یر توضیح می‌داد یک مرتبه روی چوب بست ایستاده گفت:\n",
      "Persian Prediction: اا                                                             ا    ا             یا  ااا       ا اا     ااومج ر ر میؤی ا  انا]  تايو وصاشی\"نی  ا تیسبخکقگرن2یقر2قداا حبا. جبمبيخو'شس19#[ريرلٔض.ج #ش صجگ\n",
      "-------------------------------------------\n",
      "loss at batch number 200 : 0.0007637118105776608\n",
      "English: don't play with me like a cat with a mouse.\n",
      "Persian Translation: چرا مثل گربه‌ای که با موش بازی کند با من رفتار می‌کنید\n",
      "Persian Prediction: اا                                                                                              ا        اا         ی   ا  ا اا   اوو و ا    ی  ا تربباا ارن یررن دا  ح او ا مبرخو شمرتو[ر  ؤنم2 #  ا\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, per_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, per_batch, num_heads)\n",
    "        optim.zero_grad()\n",
    "        persian_predictions = transformer(eng_batch,\n",
    "                                     per_batch,\n",
    "                                     encoder_self_attention_mask.to(device),\n",
    "                                     decoder_self_attention_mask.to(device),\n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     encoder_start_token=False,\n",
    "                                     encoder_end_token=False,\n",
    "                                     decoder_start_token=True,\n",
    "                                     decoder_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(per_batch, start_token=False, end_token=True)\n",
    "        loss = criterion(\n",
    "            persian_predictions.view(-1, persian_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == persian_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"loss at batch number {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Persian Translation: {per_batch[0]}\")\n",
    "            persian_sentence_predicted = torch.argmax(persian_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in persian_sentence_predicted:\n",
    "              if idx == persian_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += index_to_persian[idx.item()]\n",
    "            print(f\"Persian Prediction: {predicted_sentence}\")\n",
    "            print(\"-------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
